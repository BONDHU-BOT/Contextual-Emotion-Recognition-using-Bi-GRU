{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Emotion Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a_WypuUXi92e",
    "outputId": "133d026e-4236-4ff6-f21d-739bfb9640db"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, GRU, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset link: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LE6wywJrN2ih"
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "  df = pd.read_csv(filename)\n",
    "  label = df[\"label\"]\n",
    "  unique_label = list(set(label))\n",
    "  sentences = list(df[\"text\"])\n",
    "  \n",
    "  return (df, label, unique_label, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "tF0FQA7gjOCX",
    "outputId": "c609b42a-05da-49f5-8d11-bd670210f635"
   },
   "outputs": [],
   "source": [
    "df, label, unique_label, sentences = load_dataset('iseardataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disgust', 'fear', 'anger', 'joy', 'guilt', 'shame', 'sadness']\n"
     ]
    }
   ],
   "source": [
    "print(unique_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                               text Unnamed: 2\n",
      "0      joy  On days when I feel close to my partner and ot...        NaN\n",
      "1     fear  Every time I imagine that someone I love or I ...        NaN\n",
      "2    anger  When I had been obviously unjustly treated and...        NaN\n",
      "3  sadness  When I think about the short time that we live...        NaN\n",
      "4  disgust  At a gathering I found myself involuntarily si...        NaN\n",
      "5    shame  When I realized that I was directing the feeli...        NaN\n",
      "6    guilt  I feel guilty when when I realize that I consi...        NaN\n",
      "7      joy  After my girlfriend had taken her exam we went...        NaN\n",
      "8     fear  When, for the first time I realized the meanin...        NaN\n",
      "9    anger  When a car is overtaking another and I am forc...        NaN\n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.countplot(x=\"label\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O8LLUZlokg0S",
    "outputId": "c15c21dc-2ef2-43b7-b4af-e7ee9e014091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On days when I feel close to my partner and other friends.   \\nWhen I feel at peace with myself and also experience a close  \\ncontact with people whom I regard greatly.', 'Every time I imagine that someone I love or I could contact a  \\nserious illness, even death.', 'When I had been obviously unjustly treated and had no possibility  \\nof elucidating this.', 'When I think about the short time that we live and relate it to  \\nthe periods of my life when I think that I did not use this  \\nshort time.', 'At a gathering I found myself involuntarily sitting next to two  \\npeople who expressed opinions that I considered very low and  \\ndiscriminating.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "MhrziINPGHbW",
    "outputId": "0861af1b-4b82-4c92-b8f4-b6b57bb3e380"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shiningflash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/shiningflash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmNLu2YSXePb"
   },
   "outputs": [],
   "source": [
    "#define stemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-7q3iG5PKYI"
   },
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "  words = []\n",
    "  for s in sentences:\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "    w = word_tokenize(clean)\n",
    "    words.append([i.lower() for i in w])\n",
    "    \n",
    "  return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "p1j2GJgDG6qj",
    "outputId": "c7232a8e-6833-4a1d-e71a-4bc7014084a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7516\n",
      "[['on', 'days', 'when', 'i', 'feel', 'close', 'to', 'my', 'partner', 'and', 'other', 'friends', 'when', 'i', 'feel', 'at', 'peace', 'with', 'myself', 'and', 'also', 'experience', 'a', 'close', 'contact', 'with', 'people', 'whom', 'i', 'regard', 'greatly'], ['every', 'time', 'i', 'imagine', 'that', 'someone', 'i', 'love', 'or', 'i', 'could', 'contact', 'a', 'serious', 'illness', 'even', 'death']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(sentences)\n",
    "print(len(cleaned_words))\n",
    "print(cleaned_words[:2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Texts Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJCQ_YhBJW7t"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "  token = Tokenizer(filters = filters)\n",
    "  token.fit_on_texts(words)\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJhdIJC5Q3Q6"
   },
   "outputs": [],
   "source": [
    "def max_length(words):\n",
    "  return(len(max(words, key = len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JWjxPGsZZJNX",
    "outputId": "b02c8f6b-d0df-4e90-fa3a-2ff730c88300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 8989 and Maximum length = 179\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = max_length(cleaned_words)\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0TXu2xsR8jq"
   },
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "  return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dE92Hk1Va--H"
   },
   "outputs": [],
   "source": [
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyOzLEboc4LZ"
   },
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "  return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdejoJrlc-tc"
   },
   "outputs": [],
   "source": [
    "padded_doc = padding_doc(encoded_doc, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3eaSIDi0dNf1",
    "outputId": "4ab6b6dd-ffa4-4061-9e9d-7a01decfa837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded docs =  (7516, 179)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of padded docs = \",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0rXzenSpgFR"
   },
   "outputs": [],
   "source": [
    "#tokenizer with filter changed\n",
    "output_tokenizer = create_tokenizer(unique_label, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "yNHQtkszskxr",
    "outputId": "f5babc01-89e3-4392-e8e6-c9f257de3d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'anger': 3,\n",
       " 'joy': 4,\n",
       " 'guilt': 5,\n",
       " 'shame': 6,\n",
       " 'sadness': 7}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OOx9qdBto1-"
   },
   "outputs": [],
   "source": [
    "encoded_output = encoding_doc(output_tokenizer, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_5Lv5PiyG-z"
   },
   "outputs": [],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dpM86WrVQlx5",
    "outputId": "71ff52a6-b3d0-4b5c-850d-5dc0a56c8aa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7516, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD3QN-RPzfet"
   },
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "  o = OneHotEncoder(sparse = False)\n",
    "  return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6wP_Xed7RNR"
   },
   "outputs": [],
   "source": [
    "output_one_hot = one_hot(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A6HVslLTHgOM",
    "outputId": "752962df-02d8-409b-fb8f-adb06227161d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7516, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EqABUESD7xi9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8P4HTz6A4E-"
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7E0uhC2OCtTx",
    "outputId": "6ce0e215-aa3f-43f1-ba5a-0b584b25a35c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (6012, 179) and train_Y = (6012, 7)\n",
      "Shape of val_X = (1504, 179) and val_Y = (1504, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bidirectional GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5BU_x74DNEb"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "  model.add(Bidirectional(GRU(128)))\n",
    "  model.add(Dense(32, activation = \"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(7, activation = \"softmax\"))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "f-NvE0P7MFCe",
    "outputId": "8f07056b-579e-4c15-e1af-bdfa8f681e79",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 179, 128)          1150592   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               198144    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 1,357,191\n",
      "Trainable params: 206,599\n",
      "Non-trainable params: 1,150,592\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6834
    },
    "colab_type": "code",
    "id": "_r-dxm2sMQ-d",
    "outputId": "3c37b4f8-fc4e-4c82-ab46-2aa1d8b47ffd"
   },
   "outputs": [],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.9452 - accuracy: 0.1565\n",
      "Epoch 00001: val_loss improved from inf to 1.93838, saving model to model.h5\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 1.9452 - accuracy: 0.1565 - val_loss: 1.9384 - val_accuracy: 0.1868\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.9222 - accuracy: 0.1919\n",
      "Epoch 00002: val_loss improved from 1.93838 to 1.87944, saving model to model.h5\n",
      "188/188 [==============================] - 30s 157ms/step - loss: 1.9222 - accuracy: 0.1919 - val_loss: 1.8794 - val_accuracy: 0.2374\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.8723 - accuracy: 0.2399\n",
      "Epoch 00003: val_loss improved from 1.87944 to 1.84961, saving model to model.h5\n",
      "188/188 [==============================] - 31s 162ms/step - loss: 1.8723 - accuracy: 0.2399 - val_loss: 1.8496 - val_accuracy: 0.2646\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.8285 - accuracy: 0.2725\n",
      "Epoch 00004: val_loss improved from 1.84961 to 1.81347, saving model to model.h5\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 1.8285 - accuracy: 0.2725 - val_loss: 1.8135 - val_accuracy: 0.2713\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.7885 - accuracy: 0.3071\n",
      "Epoch 00005: val_loss improved from 1.81347 to 1.76444, saving model to model.h5\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 1.7885 - accuracy: 0.3071 - val_loss: 1.7644 - val_accuracy: 0.3112\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.7603 - accuracy: 0.3267\n",
      "Epoch 00006: val_loss improved from 1.76444 to 1.74935, saving model to model.h5\n",
      "188/188 [==============================] - 27s 146ms/step - loss: 1.7603 - accuracy: 0.3267 - val_loss: 1.7494 - val_accuracy: 0.3271\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.7246 - accuracy: 0.3382\n",
      "Epoch 00007: val_loss improved from 1.74935 to 1.73108, saving model to model.h5\n",
      "188/188 [==============================] - 27s 145ms/step - loss: 1.7246 - accuracy: 0.3382 - val_loss: 1.7311 - val_accuracy: 0.3298\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.7009 - accuracy: 0.3535\n",
      "Epoch 00008: val_loss improved from 1.73108 to 1.71289, saving model to model.h5\n",
      "188/188 [==============================] - 27s 146ms/step - loss: 1.7009 - accuracy: 0.3535 - val_loss: 1.7129 - val_accuracy: 0.3311\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6743 - accuracy: 0.3711\n",
      "Epoch 00009: val_loss did not improve from 1.71289\n",
      "188/188 [==============================] - 28s 152ms/step - loss: 1.6743 - accuracy: 0.3711 - val_loss: 1.7371 - val_accuracy: 0.3191\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6462 - accuracy: 0.3794\n",
      "Epoch 00010: val_loss improved from 1.71289 to 1.69525, saving model to model.h5\n",
      "188/188 [==============================] - 32s 170ms/step - loss: 1.6462 - accuracy: 0.3794 - val_loss: 1.6953 - val_accuracy: 0.3484\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6103 - accuracy: 0.4017\n",
      "Epoch 00011: val_loss improved from 1.69525 to 1.66417, saving model to model.h5\n",
      "188/188 [==============================] - 31s 166ms/step - loss: 1.6103 - accuracy: 0.4017 - val_loss: 1.6642 - val_accuracy: 0.3517\n",
      "Epoch 12/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5782 - accuracy: 0.4138\n",
      "Epoch 00012: val_loss did not improve from 1.66417\n",
      "188/188 [==============================] - 31s 164ms/step - loss: 1.5782 - accuracy: 0.4138 - val_loss: 1.6749 - val_accuracy: 0.3644\n",
      "Epoch 13/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5537 - accuracy: 0.4213\n",
      "Epoch 00013: val_loss improved from 1.66417 to 1.65436, saving model to model.h5\n",
      "188/188 [==============================] - 31s 165ms/step - loss: 1.5537 - accuracy: 0.4213 - val_loss: 1.6544 - val_accuracy: 0.3670\n",
      "Epoch 14/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5358 - accuracy: 0.4366\n",
      "Epoch 00014: val_loss improved from 1.65436 to 1.64753, saving model to model.h5\n",
      "188/188 [==============================] - 31s 167ms/step - loss: 1.5358 - accuracy: 0.4366 - val_loss: 1.6475 - val_accuracy: 0.3690\n",
      "Epoch 15/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5015 - accuracy: 0.4481\n",
      "Epoch 00015: val_loss improved from 1.64753 to 1.64574, saving model to model.h5\n",
      "188/188 [==============================] - 31s 166ms/step - loss: 1.5015 - accuracy: 0.4481 - val_loss: 1.6457 - val_accuracy: 0.3850\n",
      "Epoch 16/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4612 - accuracy: 0.4656\n",
      "Epoch 00016: val_loss improved from 1.64574 to 1.58966, saving model to model.h5\n",
      "188/188 [==============================] - 34s 182ms/step - loss: 1.4612 - accuracy: 0.4656 - val_loss: 1.5897 - val_accuracy: 0.3883\n",
      "Epoch 17/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4441 - accuracy: 0.4632\n",
      "Epoch 00017: val_loss did not improve from 1.58966\n",
      "188/188 [==============================] - 39s 207ms/step - loss: 1.4441 - accuracy: 0.4632 - val_loss: 1.6062 - val_accuracy: 0.3989\n",
      "Epoch 18/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4211 - accuracy: 0.4726\n",
      "Epoch 00018: val_loss did not improve from 1.58966\n",
      "188/188 [==============================] - 33s 174ms/step - loss: 1.4211 - accuracy: 0.4726 - val_loss: 1.6070 - val_accuracy: 0.3983\n",
      "Epoch 19/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3896 - accuracy: 0.4943\n",
      "Epoch 00019: val_loss improved from 1.58966 to 1.58049, saving model to model.h5\n",
      "188/188 [==============================] - 51s 269ms/step - loss: 1.3896 - accuracy: 0.4943 - val_loss: 1.5805 - val_accuracy: 0.4062\n",
      "Epoch 20/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3634 - accuracy: 0.4983\n",
      "Epoch 00020: val_loss improved from 1.58049 to 1.57140, saving model to model.h5\n",
      "188/188 [==============================] - 34s 183ms/step - loss: 1.3634 - accuracy: 0.4983 - val_loss: 1.5714 - val_accuracy: 0.4069\n",
      "Epoch 21/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3487 - accuracy: 0.5115\n",
      "Epoch 00021: val_loss did not improve from 1.57140\n",
      "188/188 [==============================] - 34s 183ms/step - loss: 1.3487 - accuracy: 0.5115 - val_loss: 1.5864 - val_accuracy: 0.4142\n",
      "Epoch 22/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3191 - accuracy: 0.5156\n",
      "Epoch 00022: val_loss improved from 1.57140 to 1.55276, saving model to model.h5\n",
      "188/188 [==============================] - 34s 179ms/step - loss: 1.3191 - accuracy: 0.5156 - val_loss: 1.5528 - val_accuracy: 0.4182\n",
      "Epoch 23/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2795 - accuracy: 0.5351\n",
      "Epoch 00023: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 183ms/step - loss: 1.2795 - accuracy: 0.5351 - val_loss: 1.5809 - val_accuracy: 0.4249\n",
      "Epoch 24/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2532 - accuracy: 0.5482\n",
      "Epoch 00024: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 179ms/step - loss: 1.2532 - accuracy: 0.5482 - val_loss: 1.5714 - val_accuracy: 0.4335\n",
      "Epoch 25/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2378 - accuracy: 0.5497\n",
      "Epoch 00025: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 36s 189ms/step - loss: 1.2378 - accuracy: 0.5497 - val_loss: 1.6285 - val_accuracy: 0.4229\n",
      "Epoch 26/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2097 - accuracy: 0.5612\n",
      "Epoch 00026: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 35s 185ms/step - loss: 1.2097 - accuracy: 0.5612 - val_loss: 1.6163 - val_accuracy: 0.4362\n",
      "Epoch 27/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1966 - accuracy: 0.5627\n",
      "Epoch 00027: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 172ms/step - loss: 1.1966 - accuracy: 0.5627 - val_loss: 1.5994 - val_accuracy: 0.4322\n",
      "Epoch 28/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1610 - accuracy: 0.5813\n",
      "Epoch 00028: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 35s 185ms/step - loss: 1.1610 - accuracy: 0.5813 - val_loss: 1.6647 - val_accuracy: 0.4355\n",
      "Epoch 29/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1672 - accuracy: 0.5670\n",
      "Epoch 00029: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 33s 175ms/step - loss: 1.1672 - accuracy: 0.5670 - val_loss: 1.6053 - val_accuracy: 0.4355\n",
      "Epoch 30/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1150 - accuracy: 0.5973\n",
      "Epoch 00030: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 172ms/step - loss: 1.1150 - accuracy: 0.5973 - val_loss: 1.6671 - val_accuracy: 0.4282\n",
      "Epoch 31/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0833 - accuracy: 0.6006\n",
      "Epoch 00031: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 170ms/step - loss: 1.0833 - accuracy: 0.6006 - val_loss: 1.7245 - val_accuracy: 0.4202\n",
      "Epoch 32/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0542 - accuracy: 0.6199\n",
      "Epoch 00032: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 33s 174ms/step - loss: 1.0542 - accuracy: 0.6199 - val_loss: 1.7344 - val_accuracy: 0.4422\n",
      "Epoch 33/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0196 - accuracy: 0.6219\n",
      "Epoch 00033: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 33s 177ms/step - loss: 1.0196 - accuracy: 0.6219 - val_loss: 1.7784 - val_accuracy: 0.4335\n",
      "Epoch 34/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9958 - accuracy: 0.6347\n",
      "Epoch 00034: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 183ms/step - loss: 0.9958 - accuracy: 0.6347 - val_loss: 1.7854 - val_accuracy: 0.4448\n",
      "Epoch 35/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0154 - accuracy: 0.6357\n",
      "Epoch 00035: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 33s 175ms/step - loss: 1.0154 - accuracy: 0.6357 - val_loss: 1.6758 - val_accuracy: 0.4375\n",
      "Epoch 36/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9666 - accuracy: 0.6439\n",
      "Epoch 00036: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 178ms/step - loss: 0.9666 - accuracy: 0.6439 - val_loss: 1.8552 - val_accuracy: 0.4229\n",
      "Epoch 37/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9357 - accuracy: 0.6530\n",
      "Epoch 00037: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 35s 188ms/step - loss: 0.9357 - accuracy: 0.6530 - val_loss: 1.8407 - val_accuracy: 0.4295\n",
      "Epoch 38/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9160 - accuracy: 0.6612\n",
      "Epoch 00038: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 179ms/step - loss: 0.9160 - accuracy: 0.6612 - val_loss: 1.8459 - val_accuracy: 0.4269\n",
      "Epoch 39/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8882 - accuracy: 0.6732\n",
      "Epoch 00039: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 179ms/step - loss: 0.8882 - accuracy: 0.6732 - val_loss: 2.0239 - val_accuracy: 0.4322\n",
      "Epoch 40/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8747 - accuracy: 0.6761\n",
      "Epoch 00040: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 171ms/step - loss: 0.8747 - accuracy: 0.6761 - val_loss: 2.1101 - val_accuracy: 0.4269\n",
      "Epoch 41/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8317 - accuracy: 0.6979\n",
      "Epoch 00041: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 149ms/step - loss: 0.8317 - accuracy: 0.6979 - val_loss: 2.0988 - val_accuracy: 0.4295\n",
      "Epoch 42/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8136 - accuracy: 0.6938\n",
      "Epoch 00042: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 27s 146ms/step - loss: 0.8136 - accuracy: 0.6938 - val_loss: 2.1724 - val_accuracy: 0.4269\n",
      "Epoch 43/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.6913\n",
      "Epoch 00043: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 27s 146ms/step - loss: 0.8213 - accuracy: 0.6913 - val_loss: 2.1551 - val_accuracy: 0.4302\n",
      "Epoch 44/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8042 - accuracy: 0.7021\n",
      "Epoch 00044: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 148ms/step - loss: 0.8042 - accuracy: 0.7021 - val_loss: 2.2888 - val_accuracy: 0.4229\n",
      "Epoch 45/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7735 - accuracy: 0.7109\n",
      "Epoch 00045: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 148ms/step - loss: 0.7735 - accuracy: 0.7109 - val_loss: 2.2802 - val_accuracy: 0.4176\n",
      "Epoch 46/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7639 - accuracy: 0.7139\n",
      "Epoch 00046: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 0.7639 - accuracy: 0.7139 - val_loss: 2.2815 - val_accuracy: 0.4096\n",
      "Epoch 47/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7462 - accuracy: 0.7186\n",
      "Epoch 00047: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 148ms/step - loss: 0.7462 - accuracy: 0.7186 - val_loss: 2.4362 - val_accuracy: 0.4202\n",
      "Epoch 48/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7047 - accuracy: 0.7305\n",
      "Epoch 00048: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 29s 156ms/step - loss: 0.7047 - accuracy: 0.7305 - val_loss: 2.5796 - val_accuracy: 0.4142\n",
      "Epoch 49/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6828 - accuracy: 0.7387\n",
      "Epoch 00049: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 0.6828 - accuracy: 0.7387 - val_loss: 2.6442 - val_accuracy: 0.4215\n",
      "Epoch 50/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7265 - accuracy: 0.7277\n",
      "Epoch 00050: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 148ms/step - loss: 0.7265 - accuracy: 0.7277 - val_loss: 2.3500 - val_accuracy: 0.4156\n",
      "Epoch 51/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7387 - accuracy: 0.7214\n",
      "Epoch 00051: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 149ms/step - loss: 0.7387 - accuracy: 0.7214 - val_loss: 2.4236 - val_accuracy: 0.4249\n",
      "Epoch 52/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6672 - accuracy: 0.7458\n",
      "Epoch 00052: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 151ms/step - loss: 0.6672 - accuracy: 0.7458 - val_loss: 2.7802 - val_accuracy: 0.4195\n",
      "Epoch 53/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6545 - accuracy: 0.7525\n",
      "Epoch 00053: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 29s 153ms/step - loss: 0.6545 - accuracy: 0.7525 - val_loss: 2.9663 - val_accuracy: 0.4162\n",
      "Epoch 54/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6288 - accuracy: 0.7540\n",
      "Epoch 00054: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 0.6288 - accuracy: 0.7540 - val_loss: 2.9011 - val_accuracy: 0.4122\n",
      "Epoch 55/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.7736\n",
      "Epoch 00055: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 149ms/step - loss: 0.6045 - accuracy: 0.7736 - val_loss: 2.7860 - val_accuracy: 0.4289\n",
      "Epoch 56/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5864 - accuracy: 0.7720\n",
      "Epoch 00056: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 148ms/step - loss: 0.5864 - accuracy: 0.7720 - val_loss: 2.8519 - val_accuracy: 0.4076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6227 - accuracy: 0.7636\n",
      "Epoch 00057: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 0.6227 - accuracy: 0.7636 - val_loss: 2.9346 - val_accuracy: 0.4195\n",
      "Epoch 58/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5814 - accuracy: 0.7754\n",
      "Epoch 00058: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 151ms/step - loss: 0.5814 - accuracy: 0.7754 - val_loss: 3.0307 - val_accuracy: 0.4182\n",
      "Epoch 59/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.7884\n",
      "Epoch 00059: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 149ms/step - loss: 0.5541 - accuracy: 0.7884 - val_loss: 3.0622 - val_accuracy: 0.4189\n",
      "Epoch 60/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5720 - accuracy: 0.7824\n",
      "Epoch 00060: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 0.5720 - accuracy: 0.7824 - val_loss: 3.1490 - val_accuracy: 0.4189\n",
      "Epoch 61/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5209 - accuracy: 0.7939\n",
      "Epoch 00061: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 0.5209 - accuracy: 0.7939 - val_loss: 2.9995 - val_accuracy: 0.4162\n",
      "Epoch 62/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5315 - accuracy: 0.7984\n",
      "Epoch 00062: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 148ms/step - loss: 0.5315 - accuracy: 0.7984 - val_loss: 2.9712 - val_accuracy: 0.4262\n",
      "Epoch 63/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5173 - accuracy: 0.8037\n",
      "Epoch 00063: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 27s 146ms/step - loss: 0.5173 - accuracy: 0.8037 - val_loss: 3.1873 - val_accuracy: 0.4089\n",
      "Epoch 64/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4850 - accuracy: 0.8155\n",
      "Epoch 00064: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 27s 146ms/step - loss: 0.4850 - accuracy: 0.8155 - val_loss: 3.6163 - val_accuracy: 0.4142\n",
      "Epoch 65/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4965 - accuracy: 0.8107\n",
      "Epoch 00065: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 183ms/step - loss: 0.4965 - accuracy: 0.8107 - val_loss: 3.4945 - val_accuracy: 0.4156\n",
      "Epoch 66/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4581 - accuracy: 0.8217\n",
      "Epoch 00066: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 180ms/step - loss: 0.4581 - accuracy: 0.8217 - val_loss: 3.4051 - val_accuracy: 0.4176\n",
      "Epoch 67/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.8273\n",
      "Epoch 00067: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 39s 207ms/step - loss: 0.4579 - accuracy: 0.8273 - val_loss: 3.4118 - val_accuracy: 0.4149\n",
      "Epoch 68/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.8117\n",
      "Epoch 00068: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 37s 194ms/step - loss: 0.5098 - accuracy: 0.8117 - val_loss: 3.2042 - val_accuracy: 0.4142\n",
      "Epoch 69/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.8150\n",
      "Epoch 00069: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 33s 173ms/step - loss: 0.4947 - accuracy: 0.8150 - val_loss: 3.3984 - val_accuracy: 0.4169\n",
      "Epoch 70/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4274 - accuracy: 0.8317\n",
      "Epoch 00070: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 164ms/step - loss: 0.4274 - accuracy: 0.8317 - val_loss: 3.8017 - val_accuracy: 0.4222\n",
      "Epoch 71/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8441\n",
      "Epoch 00071: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 163ms/step - loss: 0.4045 - accuracy: 0.8441 - val_loss: 3.8804 - val_accuracy: 0.4043\n",
      "Epoch 72/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4158 - accuracy: 0.8405\n",
      "Epoch 00072: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 169ms/step - loss: 0.4158 - accuracy: 0.8405 - val_loss: 3.8641 - val_accuracy: 0.4069\n",
      "Epoch 73/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4346 - accuracy: 0.8345\n",
      "Epoch 00073: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 168ms/step - loss: 0.4346 - accuracy: 0.8345 - val_loss: 3.7514 - val_accuracy: 0.4076\n",
      "Epoch 74/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4065 - accuracy: 0.8405\n",
      "Epoch 00074: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 0.4065 - accuracy: 0.8405 - val_loss: 3.6998 - val_accuracy: 0.4149\n",
      "Epoch 75/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4105 - accuracy: 0.8455\n",
      "Epoch 00075: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 0.4105 - accuracy: 0.8455 - val_loss: 4.2083 - val_accuracy: 0.4182\n",
      "Epoch 76/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7065 - accuracy: 0.7507\n",
      "Epoch 00076: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 147ms/step - loss: 0.7065 - accuracy: 0.7507 - val_loss: 3.2775 - val_accuracy: 0.4162\n",
      "Epoch 77/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5469 - accuracy: 0.8009\n",
      "Epoch 00077: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 30s 157ms/step - loss: 0.5469 - accuracy: 0.8009 - val_loss: 3.5154 - val_accuracy: 0.4156\n",
      "Epoch 78/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8385\n",
      "Epoch 00078: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 170ms/step - loss: 0.4297 - accuracy: 0.8385 - val_loss: 3.8071 - val_accuracy: 0.4182\n",
      "Epoch 79/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3922 - accuracy: 0.8506\n",
      "Epoch 00079: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 169ms/step - loss: 0.3922 - accuracy: 0.8506 - val_loss: 4.0850 - val_accuracy: 0.4169\n",
      "Epoch 80/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.8669\n",
      "Epoch 00080: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 34s 182ms/step - loss: 0.3451 - accuracy: 0.8669 - val_loss: 4.1809 - val_accuracy: 0.4176\n",
      "Epoch 81/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.8588\n",
      "Epoch 00081: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 29s 156ms/step - loss: 0.3807 - accuracy: 0.8588 - val_loss: 3.8282 - val_accuracy: 0.4182\n",
      "Epoch 82/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3530 - accuracy: 0.8673\n",
      "Epoch 00082: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 150ms/step - loss: 0.3530 - accuracy: 0.8673 - val_loss: 4.4239 - val_accuracy: 0.4249\n",
      "Epoch 83/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3249 - accuracy: 0.8796\n",
      "Epoch 00083: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 28s 146ms/step - loss: 0.3249 - accuracy: 0.8796 - val_loss: 4.5924 - val_accuracy: 0.4102\n",
      "Epoch 84/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.8629\n",
      "Epoch 00084: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 0.3642 - accuracy: 0.8629 - val_loss: 4.5836 - val_accuracy: 0.4129\n",
      "Epoch 85/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3277 - accuracy: 0.8731\n",
      "Epoch 00085: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 168ms/step - loss: 0.3277 - accuracy: 0.8731 - val_loss: 4.6225 - val_accuracy: 0.4156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.8734\n",
      "Epoch 00086: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 30s 159ms/step - loss: 0.3257 - accuracy: 0.8734 - val_loss: 4.2445 - val_accuracy: 0.4315\n",
      "Epoch 87/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3569 - accuracy: 0.8648\n",
      "Epoch 00087: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 30s 160ms/step - loss: 0.3569 - accuracy: 0.8648 - val_loss: 4.5867 - val_accuracy: 0.4129\n",
      "Epoch 88/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.8861\n",
      "Epoch 00088: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 162ms/step - loss: 0.2930 - accuracy: 0.8861 - val_loss: 4.7156 - val_accuracy: 0.4235\n",
      "Epoch 89/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2728 - accuracy: 0.8970\n",
      "Epoch 00089: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 163ms/step - loss: 0.2728 - accuracy: 0.8970 - val_loss: 4.6431 - val_accuracy: 0.4129\n",
      "Epoch 90/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.8892\n",
      "Epoch 00090: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 30s 161ms/step - loss: 0.2950 - accuracy: 0.8892 - val_loss: 4.9129 - val_accuracy: 0.4176\n",
      "Epoch 91/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8884\n",
      "Epoch 00091: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 30s 162ms/step - loss: 0.2938 - accuracy: 0.8884 - val_loss: 4.5629 - val_accuracy: 0.4202\n",
      "Epoch 92/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2748 - accuracy: 0.8930\n",
      "Epoch 00092: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 33s 174ms/step - loss: 0.2748 - accuracy: 0.8930 - val_loss: 5.2193 - val_accuracy: 0.4169\n",
      "Epoch 93/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.8718\n",
      "Epoch 00093: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 167ms/step - loss: 0.3525 - accuracy: 0.8718 - val_loss: 4.3716 - val_accuracy: 0.4162\n",
      "Epoch 94/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3279 - accuracy: 0.8784\n",
      "Epoch 00094: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 163ms/step - loss: 0.3279 - accuracy: 0.8784 - val_loss: 5.3316 - val_accuracy: 0.4189\n",
      "Epoch 95/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.8940\n",
      "Epoch 00095: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 164ms/step - loss: 0.2808 - accuracy: 0.8940 - val_loss: 5.1539 - val_accuracy: 0.4149\n",
      "Epoch 96/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9010\n",
      "Epoch 00096: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 165ms/step - loss: 0.2624 - accuracy: 0.9010 - val_loss: 5.4128 - val_accuracy: 0.4082\n",
      "Epoch 97/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8947\n",
      "Epoch 00097: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 167ms/step - loss: 0.2964 - accuracy: 0.8947 - val_loss: 5.0522 - val_accuracy: 0.4215\n",
      "Epoch 98/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.8987\n",
      "Epoch 00098: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 166ms/step - loss: 0.2692 - accuracy: 0.8987 - val_loss: 5.1637 - val_accuracy: 0.4348\n",
      "Epoch 99/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.8982\n",
      "Epoch 00099: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 31s 165ms/step - loss: 0.2715 - accuracy: 0.8982 - val_loss: 5.0955 - val_accuracy: 0.4242\n",
      "Epoch 100/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.9083\n",
      "Epoch 00100: val_loss did not improve from 1.55276\n",
      "188/188 [==============================] - 32s 169ms/step - loss: 0.2366 - accuracy: 0.9083 - val_loss: 5.4673 - val_accuracy: 0.4202\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_X, train_Y,\n",
    "                 epochs = 100,\n",
    "                 batch_size = 32,\n",
    "                 validation_data = (val_X, val_Y),\n",
    "                 callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 179, 128)          1150592   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 1,422,215\n",
      "Trainable params: 271,623\n",
      "Non-trainable params: 1,150,592\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "  model.add(Bidirectional(LSTM(128)))\n",
    "  model.add(Dense(32, activation = \"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(7, activation = \"softmax\"))\n",
    "  \n",
    "  return model\n",
    "\n",
    "model_lstm = create_model(vocab_size, max_length)\n",
    "\n",
    "model_lstm.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.9454 - accuracy: 0.1504\n",
      "Epoch 00001: val_loss improved from inf to 1.93586, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 42s 223ms/step - loss: 1.9454 - accuracy: 0.1504 - val_loss: 1.9359 - val_accuracy: 0.1822\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.9173 - accuracy: 0.1964\n",
      "Epoch 00002: val_loss improved from 1.93586 to 1.90459, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 39s 207ms/step - loss: 1.9173 - accuracy: 0.1964 - val_loss: 1.9046 - val_accuracy: 0.2367\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.8735 - accuracy: 0.2442\n",
      "Epoch 00003: val_loss improved from 1.90459 to 1.83846, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 38s 203ms/step - loss: 1.8735 - accuracy: 0.2442 - val_loss: 1.8385 - val_accuracy: 0.2819\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.8333 - accuracy: 0.2641\n",
      "Epoch 00004: val_loss improved from 1.83846 to 1.83116, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 38s 202ms/step - loss: 1.8333 - accuracy: 0.2641 - val_loss: 1.8312 - val_accuracy: 0.2799\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.8033 - accuracy: 0.2946\n",
      "Epoch 00005: val_loss improved from 1.83116 to 1.79113, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 41s 219ms/step - loss: 1.8033 - accuracy: 0.2946 - val_loss: 1.7911 - val_accuracy: 0.3138\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.7697 - accuracy: 0.3090\n",
      "Epoch 00006: val_loss improved from 1.79113 to 1.74590, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 39s 206ms/step - loss: 1.7697 - accuracy: 0.3090 - val_loss: 1.7459 - val_accuracy: 0.3424\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.7403 - accuracy: 0.3265\n",
      "Epoch 00007: val_loss did not improve from 1.74590\n",
      "188/188 [==============================] - 39s 209ms/step - loss: 1.7403 - accuracy: 0.3265 - val_loss: 1.7520 - val_accuracy: 0.3338\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.7114 - accuracy: 0.3405\n",
      "Epoch 00008: val_loss improved from 1.74590 to 1.70220, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 40s 212ms/step - loss: 1.7114 - accuracy: 0.3405 - val_loss: 1.7022 - val_accuracy: 0.3564\n",
      "Epoch 9/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.7101 - accuracy: 0.3533\n",
      "Epoch 00009: val_loss did not improve from 1.70220\n",
      "188/188 [==============================] - 40s 212ms/step - loss: 1.7101 - accuracy: 0.3533 - val_loss: 1.7238 - val_accuracy: 0.3324\n",
      "Epoch 10/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6821 - accuracy: 0.3639\n",
      "Epoch 00010: val_loss did not improve from 1.70220\n",
      "188/188 [==============================] - 40s 215ms/step - loss: 1.6821 - accuracy: 0.3639 - val_loss: 1.7372 - val_accuracy: 0.3185\n",
      "Epoch 11/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6686 - accuracy: 0.3661\n",
      "Epoch 00011: val_loss did not improve from 1.70220\n",
      "188/188 [==============================] - 41s 220ms/step - loss: 1.6686 - accuracy: 0.3661 - val_loss: 1.7133 - val_accuracy: 0.3451\n",
      "Epoch 12/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6208 - accuracy: 0.3894\n",
      "Epoch 00012: val_loss improved from 1.70220 to 1.66652, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 41s 217ms/step - loss: 1.6208 - accuracy: 0.3894 - val_loss: 1.6665 - val_accuracy: 0.3517\n",
      "Epoch 13/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6045 - accuracy: 0.3975\n",
      "Epoch 00013: val_loss did not improve from 1.66652\n",
      "188/188 [==============================] - 40s 212ms/step - loss: 1.6045 - accuracy: 0.3975 - val_loss: 1.6846 - val_accuracy: 0.3557\n",
      "Epoch 14/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6139 - accuracy: 0.3962\n",
      "Epoch 00014: val_loss did not improve from 1.66652\n",
      "188/188 [==============================] - 40s 213ms/step - loss: 1.6139 - accuracy: 0.3962 - val_loss: 1.6942 - val_accuracy: 0.3364\n",
      "Epoch 15/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.6041 - accuracy: 0.3944\n",
      "Epoch 00015: val_loss improved from 1.66652 to 1.64109, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 41s 215ms/step - loss: 1.6041 - accuracy: 0.3944 - val_loss: 1.6411 - val_accuracy: 0.3757\n",
      "Epoch 16/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5683 - accuracy: 0.4080\n",
      "Epoch 00016: val_loss did not improve from 1.64109\n",
      "188/188 [==============================] - 40s 215ms/step - loss: 1.5683 - accuracy: 0.4080 - val_loss: 1.6630 - val_accuracy: 0.3637\n",
      "Epoch 17/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5475 - accuracy: 0.4283\n",
      "Epoch 00017: val_loss improved from 1.64109 to 1.62630, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 40s 215ms/step - loss: 1.5475 - accuracy: 0.4283 - val_loss: 1.6263 - val_accuracy: 0.3790\n",
      "Epoch 18/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5236 - accuracy: 0.4333\n",
      "Epoch 00018: val_loss did not improve from 1.62630\n",
      "188/188 [==============================] - 39s 208ms/step - loss: 1.5236 - accuracy: 0.4333 - val_loss: 1.6329 - val_accuracy: 0.3783\n",
      "Epoch 19/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5874 - accuracy: 0.4192\n",
      "Epoch 00019: val_loss did not improve from 1.62630\n",
      "188/188 [==============================] - 39s 209ms/step - loss: 1.5874 - accuracy: 0.4192 - val_loss: 1.6561 - val_accuracy: 0.3677\n",
      "Epoch 20/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.5186 - accuracy: 0.4315\n",
      "Epoch 00020: val_loss did not improve from 1.62630\n",
      "188/188 [==============================] - 40s 212ms/step - loss: 1.5186 - accuracy: 0.4315 - val_loss: 1.6457 - val_accuracy: 0.3797\n",
      "Epoch 21/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4913 - accuracy: 0.4469\n",
      "Epoch 00021: val_loss did not improve from 1.62630\n",
      "188/188 [==============================] - 40s 212ms/step - loss: 1.4913 - accuracy: 0.4469 - val_loss: 1.6437 - val_accuracy: 0.3830\n",
      "Epoch 22/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4646 - accuracy: 0.4519\n",
      "Epoch 00022: val_loss did not improve from 1.62630\n",
      "188/188 [==============================] - 40s 214ms/step - loss: 1.4646 - accuracy: 0.4519 - val_loss: 1.6450 - val_accuracy: 0.3737\n",
      "Epoch 23/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4751 - accuracy: 0.4456\n",
      "Epoch 00023: val_loss did not improve from 1.62630\n",
      "188/188 [==============================] - 39s 206ms/step - loss: 1.4751 - accuracy: 0.4456 - val_loss: 1.6344 - val_accuracy: 0.3856\n",
      "Epoch 24/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4492 - accuracy: 0.4621\n",
      "Epoch 00024: val_loss did not improve from 1.62630\n",
      "188/188 [==============================] - 39s 206ms/step - loss: 1.4492 - accuracy: 0.4621 - val_loss: 1.6499 - val_accuracy: 0.4003\n",
      "Epoch 25/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4213 - accuracy: 0.4716\n",
      "Epoch 00025: val_loss did not improve from 1.62630\n",
      "188/188 [==============================] - 39s 206ms/step - loss: 1.4213 - accuracy: 0.4716 - val_loss: 1.6439 - val_accuracy: 0.3963\n",
      "Epoch 26/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4046 - accuracy: 0.4779\n",
      "Epoch 00026: val_loss improved from 1.62630 to 1.61430, saving model to model_lstm.h5\n",
      "188/188 [==============================] - 40s 211ms/step - loss: 1.4046 - accuracy: 0.4779 - val_loss: 1.6143 - val_accuracy: 0.3870\n",
      "Epoch 27/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.4033 - accuracy: 0.4804\n",
      "Epoch 00027: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 41s 217ms/step - loss: 1.4033 - accuracy: 0.4804 - val_loss: 1.6248 - val_accuracy: 0.3956\n",
      "Epoch 28/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3675 - accuracy: 0.4963\n",
      "Epoch 00028: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 46s 247ms/step - loss: 1.3675 - accuracy: 0.4963 - val_loss: 1.6497 - val_accuracy: 0.3863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3540 - accuracy: 0.5035\n",
      "Epoch 00029: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 44s 234ms/step - loss: 1.3540 - accuracy: 0.5035 - val_loss: 1.7095 - val_accuracy: 0.3850\n",
      "Epoch 30/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3552 - accuracy: 0.4960\n",
      "Epoch 00030: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 43s 228ms/step - loss: 1.3552 - accuracy: 0.4960 - val_loss: 1.6465 - val_accuracy: 0.3949\n",
      "Epoch 31/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3172 - accuracy: 0.5093\n",
      "Epoch 00031: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 47s 250ms/step - loss: 1.3172 - accuracy: 0.5093 - val_loss: 1.6255 - val_accuracy: 0.3863\n",
      "Epoch 32/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2844 - accuracy: 0.5206\n",
      "Epoch 00032: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 195ms/step - loss: 1.2844 - accuracy: 0.5206 - val_loss: 1.6401 - val_accuracy: 0.3949\n",
      "Epoch 33/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2611 - accuracy: 0.5366\n",
      "Epoch 00033: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 48s 256ms/step - loss: 1.2611 - accuracy: 0.5366 - val_loss: 1.7130 - val_accuracy: 0.3797\n",
      "Epoch 34/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2517 - accuracy: 0.5411\n",
      "Epoch 00034: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 43s 226ms/step - loss: 1.2517 - accuracy: 0.5411 - val_loss: 1.7434 - val_accuracy: 0.3976\n",
      "Epoch 35/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2285 - accuracy: 0.5426\n",
      "Epoch 00035: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 39s 209ms/step - loss: 1.2285 - accuracy: 0.5426 - val_loss: 1.7212 - val_accuracy: 0.3910\n",
      "Epoch 36/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2471 - accuracy: 0.5403\n",
      "Epoch 00036: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 197ms/step - loss: 1.2471 - accuracy: 0.5403 - val_loss: 1.7627 - val_accuracy: 0.3930\n",
      "Epoch 37/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.3133 - accuracy: 0.5165\n",
      "Epoch 00037: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 48s 253ms/step - loss: 1.3133 - accuracy: 0.5165 - val_loss: 1.6753 - val_accuracy: 0.3936\n",
      "Epoch 38/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2192 - accuracy: 0.5532\n",
      "Epoch 00038: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 1.2192 - accuracy: 0.5532 - val_loss: 1.7208 - val_accuracy: 0.4043\n",
      "Epoch 39/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1602 - accuracy: 0.5685\n",
      "Epoch 00039: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 1.1602 - accuracy: 0.5685 - val_loss: 1.7728 - val_accuracy: 0.4016\n",
      "Epoch 40/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1617 - accuracy: 0.5757\n",
      "Epoch 00040: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 193ms/step - loss: 1.1617 - accuracy: 0.5757 - val_loss: 1.7637 - val_accuracy: 0.4016\n",
      "Epoch 41/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1227 - accuracy: 0.5913\n",
      "Epoch 00041: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 189ms/step - loss: 1.1227 - accuracy: 0.5913 - val_loss: 1.8768 - val_accuracy: 0.3983\n",
      "Epoch 42/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1375 - accuracy: 0.5768\n",
      "Epoch 00042: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 193ms/step - loss: 1.1375 - accuracy: 0.5768 - val_loss: 2.0369 - val_accuracy: 0.3777\n",
      "Epoch 43/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1110 - accuracy: 0.5842\n",
      "Epoch 00043: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 192ms/step - loss: 1.1110 - accuracy: 0.5842 - val_loss: 1.8281 - val_accuracy: 0.4056\n",
      "Epoch 44/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0798 - accuracy: 0.5983\n",
      "Epoch 00044: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 191ms/step - loss: 1.0798 - accuracy: 0.5983 - val_loss: 1.8059 - val_accuracy: 0.3976\n",
      "Epoch 45/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2756 - accuracy: 0.5341\n",
      "Epoch 00045: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 194ms/step - loss: 1.2756 - accuracy: 0.5341 - val_loss: 1.7012 - val_accuracy: 0.3943\n",
      "Epoch 46/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.2646 - accuracy: 0.5339\n",
      "Epoch 00046: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 1.2646 - accuracy: 0.5339 - val_loss: 1.7900 - val_accuracy: 0.4009\n",
      "Epoch 47/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1570 - accuracy: 0.5745\n",
      "Epoch 00047: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 1.1570 - accuracy: 0.5745 - val_loss: 1.8112 - val_accuracy: 0.4003\n",
      "Epoch 48/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.1162 - accuracy: 0.5913\n",
      "Epoch 00048: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 1.1162 - accuracy: 0.5913 - val_loss: 1.8147 - val_accuracy: 0.3949\n",
      "Epoch 49/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0691 - accuracy: 0.6070\n",
      "Epoch 00049: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 1.0691 - accuracy: 0.6070 - val_loss: 1.8366 - val_accuracy: 0.3963\n",
      "Epoch 50/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0374 - accuracy: 0.6111\n",
      "Epoch 00050: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 191ms/step - loss: 1.0374 - accuracy: 0.6111 - val_loss: 1.8171 - val_accuracy: 0.3936\n",
      "Epoch 51/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0640 - accuracy: 0.6063\n",
      "Epoch 00051: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 189ms/step - loss: 1.0640 - accuracy: 0.6063 - val_loss: 1.8567 - val_accuracy: 0.4102\n",
      "Epoch 52/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0318 - accuracy: 0.6156\n",
      "Epoch 00052: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 1.0318 - accuracy: 0.6156 - val_loss: 1.9307 - val_accuracy: 0.4009\n",
      "Epoch 53/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9816 - accuracy: 0.6324\n",
      "Epoch 00053: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.9816 - accuracy: 0.6324 - val_loss: 1.9437 - val_accuracy: 0.4109\n",
      "Epoch 54/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9604 - accuracy: 0.6432\n",
      "Epoch 00054: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.9604 - accuracy: 0.6432 - val_loss: 2.0343 - val_accuracy: 0.3910\n",
      "Epoch 55/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9328 - accuracy: 0.6469\n",
      "Epoch 00055: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 192ms/step - loss: 0.9328 - accuracy: 0.6469 - val_loss: 2.1474 - val_accuracy: 0.4056\n",
      "Epoch 56/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9325 - accuracy: 0.6510\n",
      "Epoch 00056: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.9325 - accuracy: 0.6510 - val_loss: 2.0131 - val_accuracy: 0.4023\n",
      "Epoch 57/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9123 - accuracy: 0.6557\n",
      "Epoch 00057: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 195ms/step - loss: 0.9123 - accuracy: 0.6557 - val_loss: 2.2220 - val_accuracy: 0.4009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9007 - accuracy: 0.6617\n",
      "Epoch 00058: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 191ms/step - loss: 0.9007 - accuracy: 0.6617 - val_loss: 2.1213 - val_accuracy: 0.4142\n",
      "Epoch 59/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9012 - accuracy: 0.6620\n",
      "Epoch 00059: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 196ms/step - loss: 0.9012 - accuracy: 0.6620 - val_loss: 1.9507 - val_accuracy: 0.3969\n",
      "Epoch 60/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8808 - accuracy: 0.6702\n",
      "Epoch 00060: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.8808 - accuracy: 0.6702 - val_loss: 2.2631 - val_accuracy: 0.4082\n",
      "Epoch 61/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9106 - accuracy: 0.6612\n",
      "Epoch 00061: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.9106 - accuracy: 0.6612 - val_loss: 2.0874 - val_accuracy: 0.4043\n",
      "Epoch 62/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9047 - accuracy: 0.6625\n",
      "Epoch 00062: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 189ms/step - loss: 0.9047 - accuracy: 0.6625 - val_loss: 2.2813 - val_accuracy: 0.3890\n",
      "Epoch 63/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8433 - accuracy: 0.6835\n",
      "Epoch 00063: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.8433 - accuracy: 0.6835 - val_loss: 2.2580 - val_accuracy: 0.3923\n",
      "Epoch 64/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.6900\n",
      "Epoch 00064: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.8223 - accuracy: 0.6900 - val_loss: 2.3449 - val_accuracy: 0.4076\n",
      "Epoch 65/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8020 - accuracy: 0.6905\n",
      "Epoch 00065: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.8020 - accuracy: 0.6905 - val_loss: 2.3353 - val_accuracy: 0.4129\n",
      "Epoch 66/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8729 - accuracy: 0.6727\n",
      "Epoch 00066: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 191ms/step - loss: 0.8729 - accuracy: 0.6727 - val_loss: 2.2367 - val_accuracy: 0.4136\n",
      "Epoch 67/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8147 - accuracy: 0.6946\n",
      "Epoch 00067: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 194ms/step - loss: 0.8147 - accuracy: 0.6946 - val_loss: 2.2683 - val_accuracy: 0.4003\n",
      "Epoch 68/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7606 - accuracy: 0.7104\n",
      "Epoch 00068: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 189ms/step - loss: 0.7606 - accuracy: 0.7104 - val_loss: 2.4563 - val_accuracy: 0.4176\n",
      "Epoch 69/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7393 - accuracy: 0.7136\n",
      "Epoch 00069: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 38s 203ms/step - loss: 0.7393 - accuracy: 0.7136 - val_loss: 2.5247 - val_accuracy: 0.4149\n",
      "Epoch 70/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7413 - accuracy: 0.7176\n",
      "Epoch 00070: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.7413 - accuracy: 0.7176 - val_loss: 2.5366 - val_accuracy: 0.4096\n",
      "Epoch 71/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7208 - accuracy: 0.7282\n",
      "Epoch 00071: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 189ms/step - loss: 0.7208 - accuracy: 0.7282 - val_loss: 2.6403 - val_accuracy: 0.4102\n",
      "Epoch 72/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6976 - accuracy: 0.7372\n",
      "Epoch 00072: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 189ms/step - loss: 0.6976 - accuracy: 0.7372 - val_loss: 2.6669 - val_accuracy: 0.4003\n",
      "Epoch 73/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7195 - accuracy: 0.7272\n",
      "Epoch 00073: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 35s 189ms/step - loss: 0.7195 - accuracy: 0.7272 - val_loss: 2.5599 - val_accuracy: 0.4089\n",
      "Epoch 74/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6740 - accuracy: 0.7437\n",
      "Epoch 00074: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 193ms/step - loss: 0.6740 - accuracy: 0.7437 - val_loss: 2.6475 - val_accuracy: 0.4062\n",
      "Epoch 75/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9692 - accuracy: 0.6683\n",
      "Epoch 00075: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 195ms/step - loss: 0.9692 - accuracy: 0.6683 - val_loss: 2.3330 - val_accuracy: 0.4036\n",
      "Epoch 76/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7681 - accuracy: 0.7071\n",
      "Epoch 00076: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 189ms/step - loss: 0.7681 - accuracy: 0.7071 - val_loss: 2.6049 - val_accuracy: 0.4082\n",
      "Epoch 77/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.7385\n",
      "Epoch 00077: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 191ms/step - loss: 0.6838 - accuracy: 0.7385 - val_loss: 2.6418 - val_accuracy: 0.4096\n",
      "Epoch 78/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6553 - accuracy: 0.7493\n",
      "Epoch 00078: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.6553 - accuracy: 0.7493 - val_loss: 2.9193 - val_accuracy: 0.4142\n",
      "Epoch 79/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.7696\n",
      "Epoch 00079: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.6138 - accuracy: 0.7696 - val_loss: 2.9434 - val_accuracy: 0.4129\n",
      "Epoch 80/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.7385\n",
      "Epoch 00080: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.6878 - accuracy: 0.7385 - val_loss: 2.7616 - val_accuracy: 0.4089\n",
      "Epoch 81/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5887 - accuracy: 0.7718\n",
      "Epoch 00081: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.5887 - accuracy: 0.7718 - val_loss: 2.9974 - val_accuracy: 0.4089\n",
      "Epoch 82/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.7465\n",
      "Epoch 00082: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.6627 - accuracy: 0.7465 - val_loss: 2.8387 - val_accuracy: 0.3983\n",
      "Epoch 83/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.7746\n",
      "Epoch 00083: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 39s 208ms/step - loss: 0.5794 - accuracy: 0.7746 - val_loss: 2.8687 - val_accuracy: 0.4009\n",
      "Epoch 84/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.7809\n",
      "Epoch 00084: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 42s 225ms/step - loss: 0.5617 - accuracy: 0.7809 - val_loss: 3.1431 - val_accuracy: 0.4009\n",
      "Epoch 85/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5718 - accuracy: 0.7796\n",
      "Epoch 00085: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 194ms/step - loss: 0.5718 - accuracy: 0.7796 - val_loss: 3.1206 - val_accuracy: 0.4122\n",
      "Epoch 86/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.7385\n",
      "Epoch 00086: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 190ms/step - loss: 0.6938 - accuracy: 0.7385 - val_loss: 2.8433 - val_accuracy: 0.3983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6282 - accuracy: 0.7611\n",
      "Epoch 00087: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 192ms/step - loss: 0.6282 - accuracy: 0.7611 - val_loss: 2.9247 - val_accuracy: 0.3903\n",
      "Epoch 88/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5527 - accuracy: 0.7834\n",
      "Epoch 00088: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 40s 211ms/step - loss: 0.5527 - accuracy: 0.7834 - val_loss: 3.2278 - val_accuracy: 0.4043\n",
      "Epoch 89/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5499 - accuracy: 0.7904\n",
      "Epoch 00089: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 39s 206ms/step - loss: 0.5499 - accuracy: 0.7904 - val_loss: 3.1070 - val_accuracy: 0.4149\n",
      "Epoch 90/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6586 - accuracy: 0.7615\n",
      "Epoch 00090: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 193ms/step - loss: 0.6586 - accuracy: 0.7615 - val_loss: 2.8608 - val_accuracy: 0.3989\n",
      "Epoch 91/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.8024\n",
      "Epoch 00091: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 195ms/step - loss: 0.5201 - accuracy: 0.8024 - val_loss: 3.1098 - val_accuracy: 0.4069\n",
      "Epoch 92/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4997 - accuracy: 0.8006\n",
      "Epoch 00092: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 193ms/step - loss: 0.4997 - accuracy: 0.8006 - val_loss: 3.3906 - val_accuracy: 0.3989\n",
      "Epoch 93/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.8192\n",
      "Epoch 00093: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 193ms/step - loss: 0.4659 - accuracy: 0.8192 - val_loss: 3.6647 - val_accuracy: 0.4109\n",
      "Epoch 94/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5526 - accuracy: 0.7941\n",
      "Epoch 00094: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 198ms/step - loss: 0.5526 - accuracy: 0.7941 - val_loss: 2.9137 - val_accuracy: 0.4182\n",
      "Epoch 95/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4875 - accuracy: 0.8139\n",
      "Epoch 00095: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 191ms/step - loss: 0.4875 - accuracy: 0.8139 - val_loss: 3.4538 - val_accuracy: 0.4189\n",
      "Epoch 96/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.8250\n",
      "Epoch 00096: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 192ms/step - loss: 0.4470 - accuracy: 0.8250 - val_loss: 3.5088 - val_accuracy: 0.4082\n",
      "Epoch 97/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4553 - accuracy: 0.8273\n",
      "Epoch 00097: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 192ms/step - loss: 0.4553 - accuracy: 0.8273 - val_loss: 3.5112 - val_accuracy: 0.4182\n",
      "Epoch 98/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4109 - accuracy: 0.8373\n",
      "Epoch 00098: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 193ms/step - loss: 0.4109 - accuracy: 0.8373 - val_loss: 3.6034 - val_accuracy: 0.4069\n",
      "Epoch 99/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4266 - accuracy: 0.8378\n",
      "Epoch 00099: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 37s 195ms/step - loss: 0.4266 - accuracy: 0.8378 - val_loss: 3.7536 - val_accuracy: 0.4082\n",
      "Epoch 100/100\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4021 - accuracy: 0.8430\n",
      "Epoch 00100: val_loss did not improve from 1.61430\n",
      "188/188 [==============================] - 36s 193ms/step - loss: 0.4021 - accuracy: 0.8430 - val_loss: 3.6920 - val_accuracy: 0.4076\n"
     ]
    }
   ],
   "source": [
    "filename = 'model_lstm.h5'\n",
    "checkpoint = ModelCheckpoint(filename,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "hist = model_lstm.fit(train_X, train_Y,\n",
    "                 epochs = 100,\n",
    "                 batch_size = 32,\n",
    "                 validation_data = (val_X, val_Y),\n",
    "                 callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjXKos8ocXvw"
   },
   "outputs": [],
   "source": [
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSTEzrlzcuya"
   },
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "  clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "  test_word = word_tokenize(clean)\n",
    "  test_word = [w.lower() for w in test_word]\n",
    "  test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "\n",
    "  if [] in test_ls:\n",
    "    test_ls = list(filter(None, test_ls))\n",
    "    \n",
    "  test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    "  x = padding_doc(test_ls, max_length)\n",
    "\n",
    "  pred = model.predict(x)\n",
    "  \n",
    "  return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1ddofshmdzK"
   },
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "  predictions = pred[0]\n",
    "  classes = np.array(classes)\n",
    "  ids = np.argsort(-predictions)\n",
    "  classes = classes[ids]\n",
    "  predictions = -np.sort(-predictions)\n",
    " \n",
    "  for i in range(pred.shape[1]):\n",
    "    print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "  \n",
    "  return classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guilt has confidence = 0.60942477\n",
      "shame has confidence = 0.3327749\n",
      "anger has confidence = 0.028819958\n",
      "sadness has confidence = 0.01870823\n",
      "disgust has confidence = 0.005583318\n",
      "joy has confidence = 0.002525375\n",
      "fear has confidence = 0.0021633662\n",
      "\n",
      "ans: guilt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"I did not help out enough at my thesis team.\"\n",
    "pred = predictions(text)\n",
    "result = get_final_output(pred, unique_label)\n",
    "print('\\nans: {}\\n'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger has confidence = 0.4896557\n",
      "disgust has confidence = 0.22202596\n",
      "guilt has confidence = 0.14396228\n",
      "shame has confidence = 0.08689866\n",
      "sadness has confidence = 0.04276661\n",
      "joy has confidence = 0.009014194\n",
      "fear has confidence = 0.0056765988\n",
      "\n",
      "ans: anger\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"When someone stole my bike.\"\n",
    "pred = predictions(text)\n",
    "result = get_final_output(pred, unique_label)\n",
    "print('\\nans: {}\\n'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "23VpGuihMdEU",
    "outputId": "cd36c932-0fb0-4166-92ae-546a7676e645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness has confidence = 0.68341756\n",
      "joy has confidence = 0.102982506\n",
      "anger has confidence = 0.0767743\n",
      "disgust has confidence = 0.04667361\n",
      "guilt has confidence = 0.043792434\n",
      "shame has confidence = 0.029173799\n",
      "fear has confidence = 0.01718583\n",
      "\n",
      "ans: sadness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"When my girlfriend left me.\"\n",
    "pred = predictions(text)\n",
    "result = get_final_output(pred, unique_label)\n",
    "print('\\nans: {}\\n'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy has confidence = 0.63931054\n",
      "shame has confidence = 0.123251356\n",
      "sadness has confidence = 0.09643768\n",
      "guilt has confidence = 0.058223043\n",
      "fear has confidence = 0.04968705\n",
      "disgust has confidence = 0.021635186\n",
      "anger has confidence = 0.011455217\n",
      "\n",
      "ans: joy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"During the Christmas holidays, I met some of my old friends.\"\n",
    "pred = predictions(text)\n",
    "result = get_final_output(pred, unique_label)\n",
    "print('\\nans: {}\\n'.format(result))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Intent_classification_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitf6eaa932bd364e6c99622ad728a40cf7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
